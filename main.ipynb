{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test = pd.read_csv('./train_features.csv'), pd.read_csv('./test_features.csv')\n",
    "y_train, y_train_extra = pd.read_csv('./train_targets_scored.csv'), pd.read_csv('./train_targets_nonscored.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "`train_features.csv-features for training`\n",
    "- g-: specifies gene data, [4: 776]\n",
    "- c-: shows cell viability data [776:]\n",
    "- cp_type: samples treated with a compound/control vehicle\n",
    "- cp_dose: treatment dose\n",
    "- cp_time: duration\n",
    "\n",
    "`train_targets_scored`\n",
    "- They are 206 columns with unique values either 1 or 0 in each row\n",
    "- It also has nothing in common with train_targets_nonscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprecessing:\n",
    "1. There are no missing values\n",
    "2. The categorical variables have been mapped using 1 and -1, since it is more effective than binary in the neural networks.\n",
    "3. The other features (genes and cells) do not need normalizing since there is obviously some extreme values in them that will cause the model to train better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train unqiue cp_time:  [24 72 48]\n",
      "X_test unqiue cp_time:  [24 72 48]\n"
     ]
    }
   ],
   "source": [
    "# The cp_time feature in both train and test are either 24, 48, or 72\n",
    "print('X_train unqiue cp_time: ', X_train['cp_time'].unique())\n",
    "print('X_test unqiue cp_time: ', X_test['cp_time'].unique())\n",
    "\n",
    "# hence it would make sense to normalize the input with regard to mean to generate negative numbers in the column\n",
    "X_train['cp_time'] = (X_train['cp_time'] - 48) / 48\n",
    "X_test['cp_time'] = (X_test['cp_time'] - 48) / 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "\n",
    "# Dictionaries\n",
    "cp_type = {'trt_cp': 1, 'ctl_vehicle': -1}\n",
    "cp_dose = {'D1': 1, 'D2': -1}\n",
    "\n",
    "# Mapping dictionaries to columns\n",
    "X_train['cp_type'], X_test['cp_type'] = X_train['cp_type'].map(cp_type), X_test['cp_type'].map(cp_type)\n",
    "X_train['cp_dose'], X_test['cp_dose'] = X_train['cp_dose'].map(cp_dose), X_test['cp_dose'].map(cp_dose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "      <td>23814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.843285</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.248366</td>\n",
       "      <td>-0.095684</td>\n",
       "      <td>0.152253</td>\n",
       "      <td>0.081971</td>\n",
       "      <td>0.057347</td>\n",
       "      <td>-0.138836</td>\n",
       "      <td>0.035961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469244</td>\n",
       "      <td>-0.461411</td>\n",
       "      <td>-0.513256</td>\n",
       "      <td>-0.500142</td>\n",
       "      <td>-0.507093</td>\n",
       "      <td>-0.353726</td>\n",
       "      <td>-0.463485</td>\n",
       "      <td>-0.378241</td>\n",
       "      <td>-0.470252</td>\n",
       "      <td>-0.301505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.537477</td>\n",
       "      <td>0.404225</td>\n",
       "      <td>0.999818</td>\n",
       "      <td>1.393399</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>1.035731</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>1.032091</td>\n",
       "      <td>1.179388</td>\n",
       "      <td>0.882395</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000488</td>\n",
       "      <td>2.042475</td>\n",
       "      <td>2.001714</td>\n",
       "      <td>2.107105</td>\n",
       "      <td>2.159589</td>\n",
       "      <td>1.629291</td>\n",
       "      <td>2.059725</td>\n",
       "      <td>1.703615</td>\n",
       "      <td>1.834828</td>\n",
       "      <td>1.407918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-5.513000</td>\n",
       "      <td>-5.737000</td>\n",
       "      <td>-9.104000</td>\n",
       "      <td>-5.998000</td>\n",
       "      <td>-6.369000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.473075</td>\n",
       "      <td>-0.562200</td>\n",
       "      <td>-0.437750</td>\n",
       "      <td>-0.429575</td>\n",
       "      <td>-0.470925</td>\n",
       "      <td>-0.602225</td>\n",
       "      <td>-0.493900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566175</td>\n",
       "      <td>-0.565975</td>\n",
       "      <td>-0.589975</td>\n",
       "      <td>-0.568700</td>\n",
       "      <td>-0.563775</td>\n",
       "      <td>-0.567975</td>\n",
       "      <td>-0.552575</td>\n",
       "      <td>-0.561000</td>\n",
       "      <td>-0.592600</td>\n",
       "      <td>-0.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008850</td>\n",
       "      <td>-0.046600</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>-0.026900</td>\n",
       "      <td>-0.015650</td>\n",
       "      <td>-0.000650</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009900</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>-0.009100</td>\n",
       "      <td>-0.013750</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.006800</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>-0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.403075</td>\n",
       "      <td>0.663925</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.465375</td>\n",
       "      <td>0.510425</td>\n",
       "      <td>0.528725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457750</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.445675</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.444750</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.446400</td>\n",
       "      <td>0.461275</td>\n",
       "      <td>0.438650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.039000</td>\n",
       "      <td>8.257000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.282000</td>\n",
       "      <td>7.333000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.069000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>3.927000</td>\n",
       "      <td>3.596000</td>\n",
       "      <td>3.747000</td>\n",
       "      <td>2.814000</td>\n",
       "      <td>3.505000</td>\n",
       "      <td>2.924000</td>\n",
       "      <td>3.111000</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cp_type       cp_time       cp_dose           g-0           g-1  \\\n",
       "count  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \n",
       "mean       0.843285      0.000420      0.020156      0.248366     -0.095684   \n",
       "std        0.537477      0.404225      0.999818      1.393399      0.812363   \n",
       "min       -1.000000     -0.500000     -1.000000     -5.513000     -5.737000   \n",
       "25%        1.000000     -0.500000     -1.000000     -0.473075     -0.562200   \n",
       "50%        1.000000      0.000000      1.000000     -0.008850     -0.046600   \n",
       "75%        1.000000      0.500000      1.000000      0.525700      0.403075   \n",
       "max        1.000000      0.500000      1.000000     10.000000      5.039000   \n",
       "\n",
       "                g-2           g-3           g-4           g-5           g-6  \\\n",
       "count  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \n",
       "mean       0.152253      0.081971      0.057347     -0.138836      0.035961   \n",
       "std        1.035731      0.950012      1.032091      1.179388      0.882395   \n",
       "min       -9.104000     -5.998000     -6.369000    -10.000000    -10.000000   \n",
       "25%       -0.437750     -0.429575     -0.470925     -0.602225     -0.493900   \n",
       "50%        0.075200      0.008050     -0.026900     -0.015650     -0.000650   \n",
       "75%        0.663925      0.463400      0.465375      0.510425      0.528725   \n",
       "max        8.257000     10.000000     10.000000      7.282000      7.333000   \n",
       "\n",
       "       ...          c-90          c-91          c-92          c-93  \\\n",
       "count  ...  23814.000000  23814.000000  23814.000000  23814.000000   \n",
       "mean   ...     -0.469244     -0.461411     -0.513256     -0.500142   \n",
       "std    ...      2.000488      2.042475      2.001714      2.107105   \n",
       "min    ...    -10.000000    -10.000000    -10.000000    -10.000000   \n",
       "25%    ...     -0.566175     -0.565975     -0.589975     -0.568700   \n",
       "50%    ...     -0.009900      0.003250     -0.009100     -0.013750   \n",
       "75%    ...      0.457750      0.461500      0.445675      0.452900   \n",
       "max    ...      4.069000      3.960000      3.927000      3.596000   \n",
       "\n",
       "               c-94          c-95          c-96          c-97          c-98  \\\n",
       "count  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \n",
       "mean      -0.507093     -0.353726     -0.463485     -0.378241     -0.470252   \n",
       "std        2.159589      1.629291      2.059725      1.703615      1.834828   \n",
       "min      -10.000000    -10.000000    -10.000000    -10.000000    -10.000000   \n",
       "25%       -0.563775     -0.567975     -0.552575     -0.561000     -0.592600   \n",
       "50%       -0.003300     -0.010250     -0.001250     -0.006800      0.014000   \n",
       "75%        0.470900      0.444750      0.465225      0.446400      0.461275   \n",
       "max        3.747000      2.814000      3.505000      2.924000      3.111000   \n",
       "\n",
       "               c-99  \n",
       "count  23814.000000  \n",
       "mean      -0.301505  \n",
       "std        1.407918  \n",
       "min      -10.000000  \n",
       "25%       -0.562900  \n",
       "50%       -0.019500  \n",
       "75%        0.438650  \n",
       "max        3.805000  \n",
       "\n",
       "[8 rows x 875 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The breakdowns for the dataset\n",
    "\n",
    "# Training part\n",
    "cells = X_train.iloc[:,776: ].copy()\n",
    "genes = X_train.iloc[:,4:776].copy()\n",
    "cells_image = cells.values.reshape(cells.shape[0], 10, 10, 1)\n",
    "\n",
    "# Testing\n",
    "cells_test = X_test.iloc[:,776: ].copy()\n",
    "genes_test = X_test.iloc[:,4:776].copy()\n",
    "cells_image_test = cells_test.values.reshape(cells_test.shape[0], 10, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Ideas\n",
    "- Using CNNs on the cell columns: Make 10x10 dataframes and then feed it from a different part of the model and then combine the results.\n",
    "- Partitioning input: In general it is possible to break data into parts and process it differently and then aggregiate the results.\n",
    "- Instead of treating the problem as multi-label classification, get the index in the dataframe row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D, ReLU, Dropout, Flatten, Dense, InputLayer, Concatenate, SeparableConv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import TruncatedNormal, he_uniform, he_normal\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs to the NN models\n",
    "cells_input = Input(shape=(cells.shape[1]), name='Cells')\n",
    "genes_input = Input(shape=(genes.shape[1]), name='Genes')\n",
    "cell_image_input = Input(shape=(10,10,1), name='Cells_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convolutional structures used with in the CNN\n",
    "# The inception functions are inspired by the inception modules used in the inception model\n",
    "\n",
    "def inception01(input_data, mean=0, std=1):\n",
    "    # Layers 1:\n",
    "    layer11 = SeparableConv2D(256, (1,1), padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean * 0.5, stddev=std * 0.5, seed=14)\n",
    "            )(input_data)\n",
    "    \n",
    "    # Layers 2:\n",
    "    layer21 =  SeparableConv2D(512, (3,3),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 1.5, seed=15)\n",
    "            )(input_data)\n",
    "    layer22 =  SeparableConv2D(256, (1,1),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 1.5, seed=15)\n",
    "            )(layer21)\n",
    "    \n",
    "    # Layers 3:\n",
    "    layer31 =  SeparableConv2D(512, (1,1),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 2.5 + 0.1,seed=16)\n",
    "            )(input_data)\n",
    "    layer32 =  SeparableConv2D(256, (3,3),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 2.5 + 0.1, seed=16)\n",
    "            )(layer31)\n",
    "    \n",
    "    # Layers 4:\n",
    "    layer41 =  SeparableConv2D(512, (1,1),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std + 0.6,seed=17)\n",
    "            )(input_data)\n",
    "    layer42 =  SeparableConv2D(256, (5,5),padding='same',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std +0.6,seed=17)\n",
    "            )(layer41)\n",
    "\n",
    "    return Concatenate()([layer11, layer22, layer32, layer42])\n",
    "\n",
    "def inception02(input_data, mean=0, std=1):\n",
    "    # Layers 1:\n",
    "    layer11 = SeparableConv2D(128, (1,1),\n",
    "            kernel_initializer=TruncatedNormal(mean=mean * 0.5, stddev=std * 0.5, seed=14)\n",
    "            )(input_data)\n",
    "    \n",
    "    # Layers 2:\n",
    "    layer21 =  SeparableConv2D(512, (1,1), activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 1.5, seed=15)\n",
    "            )(input_data)\n",
    "    layer22 =  SeparableConv2D(128, (2, 2),\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 1.5, seed=15)\n",
    "            )(layer21)\n",
    "    \n",
    "    # Layers 3:\n",
    "    layer31 =  SeparableConv2D(512, (1,1),activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 2.5 + 0.1,seed=16)\n",
    "            )(input_data)\n",
    "    layer32 =  SeparableConv2D(256, (1, 1),activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 2.5 + 0.1, seed=16)\n",
    "            )(layer31)\n",
    "    layer32 =  SeparableConv2D(128, (3,3),\n",
    "            kernel_initializer=TruncatedNormal(mean=mean + 0.5, stddev=std * 2.5 + 0.1, seed=16)\n",
    "            )(layer32)\n",
    "    \n",
    "    # Layers 4:\n",
    "    layer41 =  SeparableConv2D(512, (1,1),activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std + 0.6,seed=17)\n",
    "            )(input_data)\n",
    "    layer42 =  SeparableConv2D(256, (1,1),activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std +0.6,seed=17)\n",
    "            )(layer41)\n",
    "    layer42 =  SeparableConv2D(256, (1,1),activation='relu',\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std +0.6,seed=17)\n",
    "            )(layer41)\n",
    "    layer42 =  SeparableConv2D(128, (5,5),\n",
    "            kernel_initializer=TruncatedNormal(mean=mean - 0.5, stddev=std +0.6,seed=17)\n",
    "            )(layer41)\n",
    "\n",
    "    return Concatenate()(\n",
    "        [Flatten()(layer11), \n",
    "         Flatten()(layer22), \n",
    "         Flatten()(layer32), \n",
    "         Flatten()(layer42)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model:\n",
    "\n",
    "def CNN_model_01(input_data):\n",
    "    \"\"\" Simple \"\"\"\n",
    "    conv1 = SeparableConv2D(256, (1,1), \n",
    "        kernel_initializer=TruncatedNormal(mean=0, stddev=0.05,seed=117)\n",
    "    )(input_data)\n",
    "    conv1 = BatchNormalization()(SeparableConv2D(256, (1, 1),\n",
    "        kernel_initializer=TruncatedNormal(mean=0, stddev=0.05,seed=118)\n",
    "    )(conv1))\n",
    "    conv1 = SeparableConv2D(512, (3,3),\n",
    "        kernel_initializer=TruncatedNormal(mean=0, stddev=0.05,seed=117), padding='same'\n",
    "    )(conv1)\n",
    "    \n",
    "    fl1 = BatchNormalization()(Dense(1000, activation='relu')(Flatten()(conv1)))\n",
    "    fl1 = Dense(1000, activation='relu')(Flatten()(fl1))\n",
    "    \n",
    "    conv2_1 = SeparableConv2D(256, (1,1), \n",
    "        kernel_initializer=TruncatedNormal(mean=10, stddev=5,seed=7)\n",
    "    )(input_data)\n",
    "    conv2_2 = BatchNormalization()(SeparableConv2D(256, (1, 1),\n",
    "        kernel_initializer=TruncatedNormal(mean=10, stddev=5,seed=8)\n",
    "    )(conv2_1))\n",
    "    conv2_3 = SeparableConv2D(512, (3,3),\n",
    "        kernel_initializer=TruncatedNormal(mean=10, stddev=5,seed=7), padding='same'\n",
    "    )(conv2_2)\n",
    "    \n",
    "    fl2 = BatchNormalization()(Dense(1000, activation='relu')(Flatten()(conv2)))\n",
    "    fl2 = Dense(1000, activation='relu')(Flatten()(fl2))\n",
    "    \n",
    "    conv3_1 = SeparableConv2D(256, (1,1), \n",
    "        kernel_initializer=TruncatedNormal(mean=-10, stddev=5,seed=7)\n",
    "    )(input_data)\n",
    "    conv3_2 = BatchNormalization()(SeparableConv2D(256, (1, 1),\n",
    "        kernel_initializer=TruncatedNormal(mean=-10, stddev=5,seed=8)\n",
    "    )(conv3_1))\n",
    "    conv3_3 = SeparableConv2D(512, (3,3),\n",
    "        kernel_initializer=TruncatedNormal(mean=-10, stddev=5,seed=7), padding='same'\n",
    "    )(conv3_2)\n",
    "    \n",
    "    fl3 = Dense(1000, activation='relu')(Flatten()(conv3))\n",
    "    fl3 = Dense(1000, activation='relu')(Flatten()(fl3))\n",
    "    \n",
    "    # Concatenating and running the fully connected layers\n",
    "    \n",
    "    \n",
    "    return main_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten_9/Reshape:0' shape=(None, 4608) dtype=float32>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model_01(cell_image_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
